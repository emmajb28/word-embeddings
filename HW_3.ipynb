{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y5cwPLm6Lyw",
        "colab_type": "text"
      },
      "source": [
        "# Homework 3: Word Embeddings\n",
        "In this homework, we will try to approximate a Skip-gram word embedding via positive pointwise mutual information (PPMI) and truncated singular value decomposition (SVD). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMnCOKC26Gzj",
        "colab_type": "text"
      },
      "source": [
        "## The setup\n",
        "Let's import the required libraries and load the data for preparing our word vectors. We are going to load a list of movie plot summaries (http://www.cs.cmu.edu/~ark/personas/) and use that as our corpus. You do not need to modify them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRKoyqtb0QL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code gets the data file from github and imports them into Colab\n",
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp20/master/HW_3/plot_summaries_tokenized.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yWaVJn30NBk",
        "colab_type": "code",
        "outputId": "0cf86efb-2ca9-4a2b-b4f2-40c87e2bbdbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from string import punctuation\n",
        "from collections import Counter, defaultdict\n",
        "from math import log2\n",
        "from scipy.sparse import csc_matrix\n",
        "from scipy.sparse.linalg import svds\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"\n",
        "    Loads the data and returns tokenized summaries.\n",
        "    \n",
        "    :return summaries_tokenized: a list that contains tokenized summaries text\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(\"plot_summaries_tokenized.csv\")\n",
        "    summaries_tokenized = list(df['SUMMARY'].apply(lambda text: text.split()))\n",
        "    return summaries_tokenized\n",
        "\n",
        "summaries = load_data()\n",
        "num_summaries = len(summaries)\n",
        "print(\"There are {} summaries.\".format(num_summaries))\n",
        "print(\"Example tokenized summary:\", summaries[1])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 42303 summaries.\n",
            "Example tokenized summary: ['The', 'nation', 'of', 'Panem', 'consists', 'of', 'a', 'wealthy', 'Capitol', 'and', 'twelve', 'poorer', 'districts', 'As', 'punishment', 'for', 'a', 'past', 'rebellion', 'each', 'district', 'must', 'provide', 'a', 'boy', 'and', 'girl', 'between', 'the', 'ages', 'of', '12', 'and', '18', 'selected', 'by', 'lottery', 'for', 'the', 'annual', 'Hunger', 'Games', 'The', 'tributes', 'must', 'fight', 'to', 'the', 'death', 'in', 'an', 'arena', 'the', 'sole', 'survivor', 'is', 'rewarded', 'with', 'fame', 'and', 'wealth', 'In', 'her', 'first', 'Reaping', '12yearold', 'Primrose', 'Everdeen', 'is', 'chosen', 'from', 'District', '12', 'Her', 'older', 'sister', 'Katniss', 'volunteers', 'to', 'take', 'her', 'place', 'Peeta', 'Mellark', 'a', 'bakers', 'son', 'who', 'once', 'gave', 'Katniss', 'bread', 'when', 'she', 'was', 'starving', 'is', 'the', 'other', 'District', '12', 'tribute', 'Katniss', 'and', 'Peeta', 'are', 'taken', 'to', 'the', 'Capitol', 'accompanied', 'by', 'their', 'frequently', 'drunk', 'mentor', 'past', 'victor', 'Haymitch', 'Abernathy', 'He', 'warns', 'them', 'about', 'the', 'Career', 'tributes', 'who', 'train', 'intensively', 'at', 'special', 'academies', 'and', 'almost', 'always', 'win', 'During', 'a', 'TV', 'interview', 'with', 'Caesar', 'Flickerman', 'Peeta', 'unexpectedly', 'reveals', 'his', 'love', 'for', 'Katniss', 'She', 'is', 'outraged', 'believing', 'it', 'to', 'be', 'a', 'ploy', 'to', 'gain', 'audience', 'support', 'as', 'sponsors', 'may', 'provide', 'inGames', 'gifts', 'of', 'food', 'medicine', 'and', 'tools', 'However', 'she', 'discovers', 'Peeta', 'meant', 'what', 'he', 'said', 'The', 'televised', 'Games', 'begin', 'with', 'half', 'of', 'the', 'tributes', 'killed', 'in', 'the', 'first', 'few', 'minutes', 'Katniss', 'barely', 'survives', 'ignoring', 'Haymitchs', 'advice', 'to', 'run', 'away', 'from', 'the', 'melee', 'over', 'the', 'tempting', 'supplies', 'and', 'weapons', 'strewn', 'in', 'front', 'of', 'a', 'structure', 'called', 'the', 'Cornucopia', 'Peeta', 'forms', 'an', 'uneasy', 'alliance', 'with', 'the', 'four', 'Careers', 'They', 'later', 'find', 'Katniss', 'and', 'corner', 'her', 'up', 'a', 'tree', 'Rue', 'hiding', 'in', 'a', 'nearby', 'tree', 'draws', 'her', 'attention', 'to', 'a', 'poisonous', 'tracker', 'jacker', 'nest', 'hanging', 'from', 'a', 'branch', 'Katniss', 'drops', 'it', 'on', 'her', 'sleeping', 'besiegers', 'They', 'all', 'scatter', 'except', 'for', 'Glimmer', 'who', 'is', 'killed', 'by', 'the', 'insects', 'Hallucinating', 'due', 'to', 'tracker', 'jacker', 'venom', 'Katniss', 'is', 'warned', 'to', 'run', 'away', 'by', 'Peeta', 'Rue', 'cares', 'for', 'Katniss', 'for', 'a', 'couple', 'of', 'days', 'until', 'she', 'recovers', 'Meanwhile', 'the', 'alliance', 'has', 'gathered', 'all', 'the', 'supplies', 'into', 'a', 'pile', 'Katniss', 'has', 'Rue', 'draw', 'them', 'off', 'then', 'destroys', 'the', 'stockpile', 'by', 'setting', 'off', 'the', 'mines', 'planted', 'around', 'it', 'Furious', 'Cato', 'kills', 'the', 'boy', 'assigned', 'to', 'guard', 'it', 'As', 'Katniss', 'runs', 'from', 'the', 'scene', 'she', 'hears', 'Rue', 'calling', 'her', 'name', 'She', 'finds', 'Rue', 'trapped', 'and', 'releases', 'her', 'Marvel', 'a', 'tribute', 'from', 'District', '1', 'throws', 'a', 'spear', 'at', 'Katniss', 'but', 'she', 'dodges', 'the', 'spear', 'causing', 'it', 'to', 'stab', 'Rue', 'in', 'the', 'stomach', 'instead', 'Katniss', 'shoots', 'him', 'dead', 'with', 'an', 'arrow', 'She', 'then', 'comforts', 'the', 'dying', 'Rue', 'with', 'a', 'song', 'Afterward', 'she', 'gathers', 'and', 'arranges', 'flowers', 'around', 'Rues', 'body', 'When', 'this', 'is', 'televised', 'it', 'sparks', 'a', 'riot', 'in', 'Rues', 'District', '11', 'President', 'Snow', 'summons', 'Seneca', 'Crane', 'the', 'Gamemaker', 'to', 'express', 'his', 'displeasure', 'at', 'the', 'way', 'the', 'Games', 'are', 'turning', 'out', 'Since', 'Katniss', 'and', 'Peeta', 'have', 'been', 'presented', 'to', 'the', 'public', 'as', 'starcrossed', 'lovers', 'Haymitch', 'is', 'able', 'to', 'convince', 'Crane', 'to', 'make', 'a', 'rule', 'change', 'to', 'avoid', 'inciting', 'further', 'riots', 'It', 'is', 'announced', 'that', 'tributes', 'from', 'the', 'same', 'district', 'can', 'win', 'as', 'a', 'pair', 'Upon', 'hearing', 'this', 'Katniss', 'searches', 'for', 'Peeta', 'and', 'finds', 'him', 'with', 'an', 'infected', 'sword', 'wound', 'in', 'the', 'leg', 'She', 'portrays', 'herself', 'as', 'deeply', 'in', 'love', 'with', 'him', 'and', 'gains', 'a', 'sponsors', 'gift', 'of', 'soup', 'An', 'announcer', 'proclaims', 'a', 'feast', 'where', 'the', 'thing', 'each', 'survivor', 'needs', 'most', 'will', 'be', 'provided', 'Peeta', 'begs', 'her', 'not', 'to', 'risk', 'getting', 'him', 'medicine', 'Katniss', 'promises', 'not', 'to', 'go', 'but', 'after', 'he', 'falls', 'asleep', 'she', 'heads', 'to', 'the', 'feast', 'Clove', 'ambushes', 'her', 'and', 'pins', 'her', 'down', 'As', 'Clove', 'gloats', 'Thresh', 'the', 'other', 'District', '11', 'tribute', 'kills', 'Clove', 'after', 'overhearing', 'her', 'tormenting', 'Katniss', 'about', 'killing', 'Rue', 'He', 'spares', 'Katniss', 'just', 'this', 'timefor', 'Rue', 'The', 'medicine', 'works', 'keeping', 'Peeta', 'mobile', 'Foxface', 'the', 'girl', 'from', 'District', '5', 'dies', 'from', 'eating', 'nightlock', 'berries', 'she', 'stole', 'from', 'Peeta', 'neither', 'knew', 'they', 'are', 'highly', 'poisonous', 'Crane', 'changes', 'the', 'time', 'of', 'day', 'in', 'the', 'arena', 'to', 'late', 'at', 'night', 'and', 'unleashes', 'a', 'pack', 'of', 'houndlike', 'creatures', 'to', 'speed', 'things', 'up', 'They', 'kill', 'Thresh', 'and', 'force', 'Katniss', 'and', 'Peeta', 'to', 'flee', 'to', 'the', 'roof', 'of', 'the', 'Cornucopia', 'where', 'they', 'encounter', 'Cato', 'After', 'a', 'battle', 'Katniss', 'wounds', 'Cato', 'with', 'an', 'arrow', 'and', 'Peeta', 'hurls', 'him', 'to', 'the', 'creatures', 'below', 'Katniss', 'shoots', 'Cato', 'to', 'spare', 'him', 'a', 'prolonged', 'death', 'With', 'Peeta', 'and', 'Katniss', 'apparently', 'victorious', 'the', 'rule', 'change', 'allowing', 'two', 'winners', 'is', 'suddenly', 'revoked', 'Peeta', 'tells', 'Katniss', 'to', 'shoot', 'him', 'Instead', 'she', 'gives', 'him', 'half', 'of', 'the', 'nightlock', 'However', 'before', 'they', 'can', 'commit', 'suicide', 'they', 'are', 'hastily', 'proclaimed', 'the', 'victors', 'of', 'the', '74th', 'Hunger', 'Games', 'Haymitch', 'warns', 'Katniss', 'that', 'she', 'has', 'made', 'powerful', 'enemies', 'after', 'her', 'display', 'of', 'defiance', 'She', 'and', 'Peeta', 'return', 'to', 'District', '12', 'while', 'Crane', 'is', 'locked', 'in', 'a', 'room', 'with', 'a', 'bowl', 'of', 'nightlock', 'berries', 'and', 'President', 'Snow', 'considers', 'the', 'situation']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ikv9DyqR7xoG",
        "colab_type": "text"
      },
      "source": [
        "We have ~42000 summaries containing ~13000000 words. We will now proceed by creating a vocabulary and will limit its size to something computationally feasible. You may find python's collections.Counter function useful. You may not import any additional libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWP4hmGG7--v",
        "colab_type": "text"
      },
      "source": [
        "# 1. Create Vocabulary\n",
        "We will start from creating our vocabulary. Vocabulary contains unigrams and their counts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ksw96WHvEoJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################\n",
        "# Do not modify\n",
        "###################\n",
        "min_count = (1 / 100) * len(summaries) #min is a hundreth of the num_summaries - 423\n",
        "max_count = (1 / 10) * len(summaries) #max is a tenth of the num_summaries - 4230"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sum1rnZN54-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def creat_vocabulary(tokenized_documents, min_count, max_count):\n",
        "    #does this take in just one document for one movie or all?\n",
        "    \"\"\"\n",
        "    This function takes in tokenized documents and returns a\n",
        "    vocabulary and word <-> index lookup dictionary of some frequently appearing words.\n",
        "    \n",
        "    :param tokenized_documents: a list of tokenized strings\n",
        "    :param min_count: minimum unigram count\n",
        "    :param max_count: maximum unigram count\n",
        "    :return vocab: a Counter where vocab[word] = count of word's occurences in all the documents\n",
        "    :return word2idx: a word -> index lookup Dictionary for words in vocab.\n",
        "    :return idx2word: a index -> word lookup Dictionary for words in vocab.\n",
        "    \"\"\"\n",
        "    # 1a. Compute unigram counts. A unigram is a single word, e.g. foo. BAG OF WORDS\n",
        "    #vocab = Counter()\n",
        "    #print(tokenized_documents[:10])\n",
        "    #vocab_movie = Counter()\n",
        "\n",
        "    vocab = Counter()\n",
        "    for movie in tokenized_documents:\n",
        "      for token in movie:\n",
        "        vocab[token] += 1\n",
        "\n",
        "    print('%d vocabs before' % len(vocab))\n",
        "    #for movie in tokenized_documents:\n",
        "      #vocab = dict(vocab) #turn vocab into regular dictionary\n",
        "      #vocab_movie = Counter(movie) #counts occur of tokens in a movie\n",
        "      #vocab = vocab_movie + vocab\n",
        "\n",
        "    # 1b. Remove unigrams that have #(unigram) < min_count or #(unigram) > max_count\n",
        "    # to eliminate unigrams occurring very frequently or infrequently. \n",
        "    # This will limit its size to something computationally feasible.\n",
        "    '''\n",
        "    for movie in tokenized_documents:\n",
        "      vocab_movie = Counter(movie)\n",
        "      for word in vocab_movie:\n",
        "        vocab[word] += vocab_movie[word]\n",
        "    '''\n",
        "    for unigram in list(vocab.items()):\n",
        "      if unigram[1] < min_count or unigram[1] > max_count:\n",
        "        del vocab[unigram[0]]\n",
        "    print('%d vocabs after' % len(vocab))\n",
        "    '''\n",
        "    vocab1 = vocab\n",
        "    for unigram,count in vocab1:\n",
        "      if count < min_count or count > max_count:\n",
        "        del vocab1[unigram]\n",
        "    '''\n",
        "    #for unigram in unigram_counts:\n",
        "  #delete a word and its count if the # of times the word appears in total is > max unigram count\n",
        "      #if (unigram_counts[unigram] < min_count) or (unigram_counts[unigram] > max_count):\n",
        "        #unigram_counts = remove_key(unigram_counts,unigram)\n",
        "    \n",
        "   \n",
        "          \n",
        "    # 1c. Build word <-> index lookup for words in vocab.\n",
        "    word2idx, idx2word = {}, {}\n",
        "    index = 0\n",
        "    for unigram in vocab.items(): #each unigram is a tuple of (\"go\":5), unigram[0] = \"go\", unigram[1] = 5\n",
        "      word2idx[unigram[0]] = index\n",
        "      index += 1\n",
        "      \n",
        "    index = 0 #reset index so all are matching\n",
        "    \n",
        "    for unigram in vocab.items():\n",
        "      idx2word[index] = unigram[0]\n",
        "      index+=1\n",
        "    print(word2idx[\"despite\"])\n",
        "    print(idx2word[3])\n",
        "\n",
        "\n",
        "    return vocab, word2idx, idx2word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fMyLhDCktKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_key(dictionary, key):\n",
        "    r = dict(dictionary)\n",
        "    del r[key]\n",
        "    return r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71G0q8l_51CH",
        "colab_type": "code",
        "outputId": "b80892b9-12ef-4bfa-aa35-84c3daf314c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "vocab, word2idx, idx2word = creat_vocabulary(summaries, min_count, max_count)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "214147 vocabs before\n",
            "2750 vocabs after\n",
            "3\n",
            "despite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NwfhahT_tRd",
        "colab_type": "text"
      },
      "source": [
        "# 2. Build Term-Context Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ-tvqGE1ykI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################\n",
        "# Do not modify\n",
        "###################\n",
        "window_size = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQvXB-MZ_VqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_term_context_matrix(tokenized_documents, vocab, window_size):\n",
        "    \"\"\"\n",
        "    This function returns a `word_pair_count` Counter with each \n",
        "    word_pair_count[(w, c)] = number of times the word `c` occurs in the context of word `w`. (where `w`, `c` belong to the vocab)\n",
        "    To make it efficient, instead of building the sparse term-context matrix, \n",
        "    we will build 3 separate Counters: word_pair_count, w_count, c_count\n",
        "    You may find python's Counter useful here\n",
        "\n",
        "    :param tokenized_documents: a list of tokenized strings\n",
        "    :param vocab: vocabulary Counter\n",
        "    :param window_size: context window size\n",
        "    :return word_pair_count: a Counter where word_pair_count[(w, c)] = count of c's occurences in w's context window, i.e. #(w, c)\n",
        "    :return w_count: a Counter where w_count[w] = the number of times w occured in the documents, i.e. #(w)\n",
        "    :return c_count: a Counter where c_count[c] = the number of times c occured in the documents, i.e. #(c)\n",
        "    \"\"\"\n",
        "    word_pair_count = Counter()  \n",
        "    w_count = Counter()\n",
        "    c_count = Counter()\n",
        "    target = \"\"\n",
        "   \n",
        "    count = 0\n",
        "    for movie in tokenized_documents:\n",
        "      for index in range(len(movie)):\n",
        "        target = movie[index]\n",
        "        if target in vocab: #if target not in vocab, go to next index for a target\n",
        "          w_count[target] += 1\n",
        "          for step in range(1,window_size+1):\n",
        "            if (index-step) >= 0:\n",
        "              if movie[index-step] in vocab:\n",
        "                #context_left.append(movie[index-step])\n",
        "                c_count[movie[index-step]] += 1\n",
        "                word_pair_count[(target,movie[index-step])] += 1\n",
        "            if (index+step) <= (len(movie)-1):\n",
        "              if movie[index+step] in vocab:\n",
        "                #context_right.append(movie[index+step])\n",
        "                c_count[movie[index+step]] += 1\n",
        "                word_pair_count[(target,movie[index+step])] += 1\n",
        "          '''\n",
        "          context_words.append(context_left)\n",
        "          context_words.append(context_right)\n",
        "          for c in context_words:\n",
        "            word_pair_count[(target,c)] += 1\n",
        "          '''\n",
        "\n",
        "\n",
        "\n",
        "          '''\n",
        "          if index-step => 0 and index+step <= (len(movie)-1):\n",
        "            if movie[index-step] in vocab:\n",
        "              context_words.append(movie[index-step])\n",
        "              c_count[movie[index-step]] += 1\n",
        "              word_pair_count[(target,movie[index-step])] += 1\n",
        "            if movie[index+step] in vocab:\n",
        "              context_words.append(movie[index+step])\n",
        "              c_count[movie[index+step]] += 1\n",
        "              word_pair_count[(target,movie[index+step])] += 1\n",
        "          '''\n",
        "          '''\n",
        "\n",
        "          for window_location in range(1,window_size+1):\n",
        "            if (index - window_location) < 0:\n",
        "              #if movie[index+window_location] not in vocab:\n",
        "                #continue\n",
        "              #else:\n",
        "              context_words.append(movie[index+window_location])\n",
        "              c_count[movie[index+window_location]] += 1\n",
        "              #continue\n",
        "            if (index + window_location) > (len(movie)-1):\n",
        "              #if movie[index-window_location] not in vocab:\n",
        "                #continue\n",
        "              #else:\n",
        "              context_words.append(movie[index-window_location])\n",
        "              c_count[movie[index-window_location]] += 1\n",
        "              #continue\n",
        "            else:\n",
        "              context_words.append(movie[index-window_location]) #left\n",
        "              context_words.append(movie[index+window_location]) #right\n",
        "          for c in context_words:\n",
        "            word_pair_count[(target,c)] += 1\n",
        "    '''        \n",
        "    return word_pair_count, w_count, c_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXOfuq08Kqws",
        "colab_type": "code",
        "outputId": "dc0d2400-5ff5-4583-a9bf-8e169cb473d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "word_pair_count"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-646be17568df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_pair_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'word_pair_count' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBxi0t1y2jQ_",
        "colab_type": "code",
        "outputId": "ad9ad885-5551-40e9-9274-d25583e3feeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word_pair_count, w_count, c_count = build_term_context_matrix(summaries, vocab, window_size)\n",
        "print(\"There are {} word-context pairs\".format(len(word_pair_count)))\n",
        "\n",
        "# The number of w_count and c_count should match your number of vocab\n",
        "assert len(w_count) == len(vocab)\n",
        "assert len(c_count) == len(vocab)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1917545 word-context pairs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeaZoasi3m5r",
        "colab_type": "text"
      },
      "source": [
        "# 3. Build Positive Pointwise Mutual Information (PPMI) Matrix\n",
        "In this part, you will build a PPMI matrix using Scipy's Compressed Sparse Column matrix to save storage space. (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)\n",
        "\n",
        "Sparse matrix is a matrix which contains very few non-zero elements. When a sparse matrix is represented with a 2-dimensional array, we waste a lot of space to represent that matrix. In NLP application, it's quite common to use sparse matrix since the size of vocabulary is usually very large. \n",
        "\n",
        "Below is an example of how to build a sparse matrix where `data`, `row` and `col` satisfy the relationship `M[row[k], col[k]] = data[k]`.\n",
        "\n",
        "```python\n",
        ">>> row = np.array([0, 2, 2, 0, 1, 2])\n",
        ">>> col = np.array([0, 0, 1, 2, 2, 2])\n",
        ">>> data = np.array([1, 2, 3, 4, 5, 6])\n",
        ">>> M = csc_matrix((data, (row, col)))\n",
        ">>> M.toarray()\n",
        "array([[1, 0, 4],\n",
        "       [0, 0, 5],\n",
        "       [2, 3, 6]])\n",
        "```\n",
        "\n",
        "Recall that\n",
        "$$\n",
        "\\begin{gather*}\n",
        "  \\text{PMI}(w, c) = \\log_2 \\frac{P(w, c)}{P(w)P(c)} \\\\\n",
        "  \\text{PPMI}(w, c) = \\max(0, \\text{PMI}(w, c))\n",
        "\\end{gather*}\n",
        "$$\n",
        "You should use `log2` function from the math package that is alreadly imported for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIYharDm38G1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_PPMI_matrix(word_pair_count, w_count, c_count, word2idx):\n",
        "    \"\"\"\n",
        "    This function returns a PPMI matrix represented by a csc sparse matrix.\n",
        "\n",
        "    :params word_pair_count: a Counter where word_pair_count[(w, c)] = count of c's occurences in w's context window\n",
        "    :return w_count: a Counter where w_count[w] = the number of times w occured in the documents\n",
        "    :return c_count: a Counter where c_count[c] = the number of times c occured in the documents\n",
        "    :return word2idx: a word -> index lookup Dictionary for words in vocab\n",
        "    :return PPMI: PPMI csc sparse matrix\n",
        "    \"\"\"\n",
        "    data, rows, cols = [], [], []\n",
        "    #data is ppmi for each pair of row and col\n",
        "    total_occurences = sum(word_pair_count.values())\n",
        "    for (w, c), n in word_pair_count.items():\n",
        "      rows.append(word2idx[w])\n",
        "      cols.append(word2idx[c])\n",
        "      top = word_pair_count[(w,c)]/total_occurences\n",
        "      bottom = (c_count[c]/total_occurences)*(w_count[w]/total_occurences)\n",
        "      pmi = top/bottom\n",
        "      pmi = log2(pmi)\n",
        "      ppmi = max(0,pmi)\n",
        "      data.append(ppmi)\n",
        "    PPMI = csc_matrix((data, (rows, cols)))\n",
        "    return PPMI"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3QIgF_GQ6mX",
        "colab_type": "code",
        "outputId": "83a67a7a-69da-4de0-9e3c-f850434cef70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "PPMI.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2750, 2750)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV0ZuaD_UUK8",
        "colab_type": "code",
        "outputId": "8c2d6804-7f53-4159-cc4e-0c73eab1b830",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sum(word_pair_count.values())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3872596"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADuP5FPV8-XQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PPMI = build_PPMI_matrix(word_pair_count, w_count, c_count, word2idx)\n",
        "\n",
        "# The shape of PPMI matrix should match your number of vocab\n",
        "assert PPMI.shape == (len(vocab), len(vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLUHCDzN9PGF",
        "colab_type": "text"
      },
      "source": [
        "# 4. Truncated SVD\n",
        "In this part, we will obtain a dense low-dimensional vectors via truncated (rank-k) SVD. You should use `svds` function from Sicpy that is already imported for you to obtain the SVD factorization.\n",
        "(https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.svds.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEh5rynC9-UR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################\n",
        "# Do not modify\n",
        "###################\n",
        "rank = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLtCNz5Z9U8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embeddings(PPMI, rank):\n",
        "    \"\"\"\n",
        "    Reutrns the left singular vectors as word embeddings via truncated SVD\n",
        "\n",
        "    :params PPMI: PPMI csc sparse matrix\n",
        "    :params rank: number of singular values and vectors to compute\n",
        "    :return u: left sigular vectors from sprase SVD\n",
        "    :return s: the singular values from sparse SVD\n",
        "    \"\"\"\n",
        "    u,s,vt = svds(PPMI, rank)\n",
        "    return u, s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmjoP5KF91O0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "embeddings, _ = get_embeddings(PPMI, rank)\n",
        "\n",
        "# The shape of the embeddings matrix should be (# vocab, rank)\n",
        "assert embeddings.shape == (len(vocab), rank)\n",
        "'''\n",
        "embeddings, _ = get_embeddings(PPMI, rank)\n",
        "embeddings /= np.linalg.norm(embeddings, axis=1, keepdims=True)  # Normalize embeddings matrix\n",
        "\n",
        "# The shape of the embeddings matrix should be (# vocab, rank)\n",
        "assert embeddings.shape == (len(vocab), rank)\n",
        "\n",
        "# Make sure embeddings is normalized\n",
        "assert True == np.isclose(np.linalg.norm(embeddings[0]), 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQUUfS0N-Lyc",
        "colab_type": "text"
      },
      "source": [
        "# 5. Evaluate Word Embeddings via Cosine Similarity\n",
        "\n",
        "Using cosine similarity as a measure of distance [ยง6.4 Jurafsky & Martin](https://web.stanford.edu/~jurafsky/slp3/6.pdf), we will now find the closest words to a certain word. We define cosine similarity as, $$cosine(\\overrightarrow{v},\\overrightarrow{w}) = \\frac{\\overrightarrow{v} \\cdot \\overrightarrow{w}}{\\vert v \\vert \\vert w \\vert}$$\n",
        "\n",
        "Please complete the function below that calculates the 'K' closest words from the vocabulary. You may not use any additional libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9Zf_us2AFkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################\n",
        "# Do not modify\n",
        "###################\n",
        "num_neighbors = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_l55j98-NvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine_distances(matrix, vector):\n",
        "    \"\"\"\n",
        "    The function takes in a matrix and a vector (both normalized) \n",
        "    and returns the cosine distances for this vector against all others.\n",
        "    The pretrained embeddings are normalized.\n",
        "\n",
        "    :params matrix: word embeddings matrix\n",
        "    :params vector: word vector for a particular word\n",
        "    :return distances: a cosine distances vector\n",
        "    \"\"\"\n",
        "    distances = []\n",
        "    for row in matrix:\n",
        "      top = np.dot(vector,row)\n",
        "      bottom = np.linalg.norm(vector)*np.linalg.norm(row)\n",
        "      cosine = top/bottom\n",
        "      distances.append(cosine)\n",
        "\n",
        "    \n",
        "    return  distances\n",
        "\n",
        "\n",
        "def nearest_neighbors(embeddings, word, k, word2idx, idx2word):\n",
        "    \"\"\"\n",
        "    For each query word, this function returns the k closest words from the vocabulary.\n",
        "\n",
        "    :params embeddings: word embedding matrix\n",
        "    :params word: query word\n",
        "    :params k: number of closest words to return\n",
        "    :params word2idx: a word -> index lookup dictionary\n",
        "    :params idx2word: a index -> word lookup dictionary\n",
        "    :return nearest_neighbors: a list of cloest words\n",
        "    \"\"\"\n",
        "    vector = embeddings[word2idx[word]]\n",
        "    distances = cosine_distances(embeddings, vector)\n",
        "    #nearest_neighbors = []\n",
        "    \n",
        "    rank_distances = np.argsort(distances)[-(k+1):][::-1]\n",
        "    #index = 0\n",
        "    nearest_neighbors = []\n",
        "    for d in range(len(rank_distances)):\n",
        "      if idx2word[rank_distances[d]] == word:\n",
        "        continue\n",
        "      else:\n",
        "        nearest_neighbors.append(idx2word[rank_distances[d]])\n",
        "\n",
        "\n",
        "    return nearest_neighbors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJjPuVPe_oGq",
        "colab_type": "code",
        "outputId": "09a43d18-2d42-40a2-d336-67fc733452b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "query_words = [\"doctor\", \"zombie\", \"robot\", \"eat\", \"bus\"]\n",
        "for word in query_words:\n",
        "    print(word, nearest_neighbors(embeddings, word, num_neighbors, word2idx, idx2word))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "doctor ['Anna', 'priest', 'nurse', 'teacher', 'Elizabeth']\n",
            "zombie ['infected', 'creatures', 'zombies', 'vampires', 'possessed']\n",
            "robot ['alien', 'weapon', 'demon', 'creature', 'virus']\n",
            "eat ['throw', 'sleep', 'stand', 'wait', 'wear']\n",
            "bus ['road', 'boat', 'truck', 'station', 'airport']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfxaNuBjAiiY",
        "colab_type": "text"
      },
      "source": [
        "# 6. Evaluate Word Embeddings via Analogous Tasks\n",
        "\n",
        "The embedding space is known to capture the semantic context of words. An example of it is $\\overrightarrow{woman} - \\overrightarrow{man} \\simeq \\overrightarrow{queen} - \\overrightarrow{king}$. Use the `cosine_distances()` function you wrote above to find such relations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZQCzP-FCRb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relation(embeddings, query_words, word2idx, idx2word):\n",
        "    \"\"\"\n",
        "    Takes in 3 words and returns the closest word (in terms of cosine similarity)\n",
        "    to the normalized algebraic addition of the three vectors.\n",
        "    The parameters follow this order : word_vec1 - word_vec2 ~ closest - word_vec3\n",
        "\n",
        "    :params embeddings: word embedding matrix\n",
        "    :params query_words: a list of query words in the following order: [word1, word2, word3]\n",
        "    :params word2idx: a word -> index lookup dictionary\n",
        "    :params idx2word: a index -> word lookup dictionary\n",
        "    :return closet_word: the closest word for the relation\n",
        "    \"\"\"\n",
        "    #want to get vectors from embeddings matrix and then add them together and subtract the one and then get words for the nearest vector\n",
        "    word1, word2, word3 = query_words\n",
        "    three_vectors = []\n",
        "    if all(word in vocab for word in query_words):\n",
        "      for words in query_words:\n",
        "\n",
        "        vector = embeddings[word2idx[words]]\n",
        "        three_vectors.append(vector)\n",
        "      three_vectors = three_vectors[0] - three_vectors[1] + three_vectors[2]\n",
        "      three_vectors /= np.linalg.norm(three_vectors)\n",
        "      distances = cosine_distances(embeddings,three_vectors)\n",
        "      rank_distances = np.argsort(distances)\n",
        "      \n",
        "      #closet_word = rank_distances[len(rank_distances)-1]\n",
        "      closet_word = idx2word[rank_distances[len(rank_distances)-1]]\n",
        "      if closet_word == word1 or closet_word == word2 or closet_word == word3:\n",
        "        closet_word = idx2word[rank_distances[len(rank_distances)-2]]\n",
        "        \n",
        "\n",
        "      return closet_word\n",
        "    else:\n",
        "      missing = [w for w in query_words if w not in vocab]\n",
        "      raise Exception(\"missing {} from vocabulary\".format(\", \".join(missing)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF3mtHMjHue-",
        "colab_type": "code",
        "outputId": "fe6d5edb-8560-4a23-852f-0e2fd71ed255",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "queries = [[\"doctor\", \"nurse\", \"king\"], [\"robot\", \"weapon\", \"bus\"], [\"sing\", \"song\", \"justice\"], [\"elderly\", \"kids\", \"teenager\"], [\"soldier\", \"wound\", \"telephone\"]]\n",
        "for query in queries:\n",
        "  closet_word = relation(embeddings, query, word2idx, idx2word)\n",
        "  print(\"{} - {} ~= {} - {}\".format(query[0], query[1], closet_word, query[2]))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "doctor - nurse ~= Lord - king\n",
            "robot - weapon ~= road - bus\n",
            "sing - song ~= defend - justice\n",
            "elderly - kids ~= widow - teenager\n",
            "soldier - wound ~= post - telephone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMh591xQ7-Rx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}